{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "CS 589: Final Project\n",
    "@authors: Emma Kearney, Van Nguyen\n",
    "'''\n",
    "\n",
    "\n",
    "# Load libraries ---------------------\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import cluster_class \n",
    "\n",
    "\n",
    "# Sklearn machine learning model\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Model selection, evaluation metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "\n",
    "\n",
    "# Load data ---------------------\n",
    "# Define data path, column names\n",
    "DATA_PATH = '/Users/VanNguyen/Desktop/COMPSCI589/Final Project/Data/'\n",
    "\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(DATA_PATH + 'small_train.csv')\n",
    "# test = pd.read_csv(DATA_PATH + 'test_train.csv')\n",
    "\n",
    "   \n",
    "\n",
    "# Data Processing --------------------\n",
    "\n",
    "def make_dummy(data):\n",
    "    '''\n",
    "    This function takes a full data set, converts cetegorical data to numerical data,\n",
    "    and split data inputs and data outputs. \n",
    "\n",
    "    Args: \n",
    "        data: A data matrix of dimension (N,D+1).\n",
    "\n",
    "    Returns:\n",
    "        X: A data matrix X (N, D).\n",
    "        y: A vector label (N, 1).\n",
    "    '''\n",
    "\n",
    "    # Create indicator variables for categorical features. \n",
    "    season_dummy = np.array(pd.get_dummies(data.loc[:, 'season']))\n",
    "    holiday_dummy = np.array(pd.get_dummies(data.loc[:, 'holiday']))\n",
    "    workingday_dummy = np.array(pd.get_dummies(data.loc[:, 'workingday']))\n",
    "    weathersit_dummy = np.array(pd.get_dummies(data.loc[:, 'weathersit']))\n",
    "    station_dummy = np.array(pd.get_dummies(data.loc[:, 'station']))\n",
    "    type_dummy = np.array(pd.get_dummies(data.loc[:, 'type']).iloc[:, 1])\n",
    "\n",
    "    # Delete categorical columns.\n",
    "    data.drop(['season', 'holiday', 'workingday', \n",
    "        'weathersit', 'station', 'type'], axis = 1, inplace = True)\n",
    "\n",
    "    # Column bind the dummy matrix to the data matrix.\n",
    "    data = np.column_stack((np.array(data), season_dummy, station_dummy, type_dummy))\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "# K-Means Cross Validation --------------------\n",
    "\n",
    "def KMeans_CV(X, K_vals):   \n",
    "    '''\n",
    "    This function does cross validation for K-Means clustering model.\n",
    "\n",
    "    Args:\n",
    "        X: A data matrix X of dimension (N, D).\n",
    "        K_vals: A range of hyperparameter values to search over.\n",
    "\n",
    "    Returns:\n",
    "        The optimal hyperparameter n_clusters of K-Means. \n",
    "    ''' \n",
    "\n",
    "    # Declare the range of hyperparamters to search over\n",
    "    params = {'n_clusters': K_vals}\n",
    "\n",
    "    # Initialize a K-Means model\n",
    "    km = KMeans(random_state = 0)\n",
    "\n",
    "    # Fit K-Means model and search for the best hyperparameter based on\n",
    "    # K-Means default scoring method.\n",
    "    km = GridSearchCV(km, params)\n",
    "    km.fit(X)\n",
    "\n",
    "    return km.best_estimator_\n",
    "\n",
    "\n",
    "# Decision Tree Cross Validation --------------------\n",
    "\n",
    "def dt_CV(k, depth_vals, X_train, y_train, create_plot = None):\n",
    "    '''\n",
    "    This functions runs cross validation for a Decision Tree classifier.\n",
    "\n",
    "    Args:\n",
    "        k: A positive number determining the number of fold in K-Fold cross validation.\n",
    "        depth_vals: A list of values for depth of the DecisionTreeClassifier.\n",
    "        X_train: A data matrix (N, D) for training and validation.\n",
    "        y_train: A vector label (N, 1) for training output labels.\n",
    "        create_plot: Default 'None'. If 'True', produce a plot of training and validation RMSE curves.\n",
    "\n",
    "    Returns:\n",
    "        The optimal DecisionTreeClassifier\n",
    "        and an optinal plot of training and validation RMSE curves. \n",
    "    '''\n",
    "\n",
    "    # We use negative mean squared error to account for scoring mechanism in sklearn.\n",
    "    # cv = k sets the number of folds in k-fold cross validation. \n",
    "\n",
    "#     dt = DecisionTreeClassifier(random_state = 0)\n",
    "    train_scores, valid_scores = validation_curve(DecisionTreeClassifier(random_state = 0),\n",
    "                                                  X_train, y_train, \n",
    "                                                  param_name = \"max_depth\", \n",
    "                                                  param_range = depth_vals, \n",
    "                                                  cv = k, \n",
    "                                                  scoring = 'neg_mean_squared_error') \n",
    "    # Convert negative mse to useable rmse values.\n",
    "    train_mse = abs(train_scores)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "    valid_mse = abs(valid_scores)\n",
    "    valid_rmse = np.sqrt(valid_mse)\n",
    "\n",
    "    # Find indices of minimum CV RMSE.\n",
    "    # Use the row index from above to select the corresponding alpha value.\n",
    "    min_valid_rmse = np.unravel_index(valid_rmse.argmin(), valid_rmse.shape)\n",
    "    optimal_depth = depth_vals[min_valid_rmse[0]]\n",
    "    \n",
    "    # Initialize a DecisionTreeClassifier with optimal max_depth and retrain data.\n",
    "    optimal_dt = DecisionTreeClassifier(max_depth = optimal_depth) \n",
    "    optimal_dt.fit(X_train, y_train)\n",
    "\n",
    "    # Option to produce a train/validation plot.\n",
    "    if create_plot:\n",
    "        plot_validation_curve(train_rmse, valid_rmse, depth_vals)\n",
    "\n",
    "    return optimal_dt\n",
    "\n",
    "\n",
    "def plot_validation_curve(train_rmse, valid_rmse, depth_vals):\n",
    "    '''\n",
    "    This is an utility function for dt_CV().\n",
    "    It produces a train/validation curves plot during Decision Tree cross validation.\n",
    "    '''\n",
    "    \n",
    "    # since each iteration of a new alpha value yields 5 folds,\n",
    "    # take the average rmse at each alpha level\n",
    "    train_rmse_mean = np.mean(train_rmse, axis=1)\n",
    "    train_rmse_std = np.std(train_rmse, axis=1)\n",
    "    valid_rmse_mean = np.mean(valid_rmse, axis=1)\n",
    "    valid_rmse_std = np.std(valid_rmse, axis=1)\n",
    "    \n",
    "    # used plotting example from sklearn validation_curve documentation\n",
    "    # as outline for code below\n",
    "    plt.title(\"Train-Validation Curve with KFold-CV on Decision Tree Classifier\")\n",
    "    plt.xlabel(\"Maximum Tree Depth\") \n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.ylim(0.0, np.amax(valid_rmse)+2)\n",
    "    plt.xlim(0.0, np.amax(depth_vals)+10)\n",
    "    lw = 2\n",
    "    plt.plot(depth_vals, train_rmse_mean, label=\"Training RMSE\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "    plt.fill_between(depth_vals, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.2,\n",
    "                     color=\"darkorange\", lw=lw)\n",
    "    plt.plot(depth_vals, valid_rmse_mean, label=\"Cross-validation RMSE\",\n",
    "             color=\"navy\", lw=lw)\n",
    "    plt.fill_between(depth_vals, valid_rmse_mean - valid_rmse_std,\n",
    "                     valid_rmse_mean + valid_rmse_std, alpha=0.2,\n",
    "                     color=\"navy\", lw=lw)\n",
    "    plt.legend(loc=\"best\")\n",
    "    # Save plot to folder\n",
    "    plt.savefig('../Figures/train_val_curve.pdf')\n",
    "\n",
    "\n",
    "\n",
    "# SVM Cross Validation --------------------\n",
    "\n",
    "def svm_CV(k, C_vals, kernel_vals, X_train, y_train):\n",
    "    '''\n",
    "    This functions runs cross validation for an SVM classifier.\n",
    "\n",
    "    Args:\n",
    "        k: A positive number determining the number of fold in K-Fold cross validation.\n",
    "        C_vals: Penalty parameter C of the error term.\n",
    "        kernel_vals: The kernel types to be used in the algorithm.\n",
    "        X_train: A data matrix (N, D) for training and validation.\n",
    "        y_train: A vector label (N, 1) for training output labels.\n",
    "\n",
    "    Returns:\n",
    "        The optimal SVC classifier.\n",
    "    '''\n",
    "\n",
    "    # Declare the range of hyperparamters to search over\n",
    "    params = {'C': C_vals, 'kernel': kernel_vals}\n",
    "    \n",
    "    # Initialize an SVC classifier\n",
    "    svc = SVC()\n",
    "\n",
    "    # Fit SVC model and search for the best hyperparameter based on\n",
    "    # SVC default scoring method.\n",
    "\n",
    "    svc = GridSearchCV(svc, params, cv = k)\n",
    "    svc.fit(X_train, y_train)\n",
    "    \n",
    "    return svc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=4, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=0, tol=0.0001, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Make data.\n",
    "X_train, y_train = make_dummy(train)\n",
    "X_test, y_test = make_dummy(test)\n",
    "\n",
    "\n",
    "# Part 2: K-Means CV.   \n",
    "K_vals = range(1, 5)\n",
    "best_km = KMeans_CV(X = X_train, K_vals = K_vals)\n",
    "# Note down best K here.\n",
    "print best_km\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Part 3: Fit pipeline.\n",
    "\n",
    "best_K = 3                                    # CHANGE THIS \n",
    "\n",
    "# Part 3.1: Fit baseline model.\n",
    "clf = cluster_class.Cluster_Class(K = best_K, r = 0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (0.73315, 0.26685), 1: (0.09091, 0.90909), 2: (0.02083, 0.97917)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_baseline(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: (0.82725, 0.17275), 1: (0.0, 1.0), 2: (0.0, 1.0)}\n"
     ]
    }
   ],
   "source": [
    "# Part 3.2: Fit cluster-classification model. Tune each classifier.\n",
    "import random\n",
    "\n",
    "# Part 3.2.1: Tune DecisionTreeClassifier.\n",
    "depth_vals = range(1, 10)          # CHANGE THIS\n",
    "\n",
    "clf = cluster_class.Cluster_Class(K = best_K, r = 0)\n",
    "clf.fit(X_train, y_train, True)\n",
    "\n",
    "\n",
    "for i in xrange(clf.K):\n",
    "    \n",
    "    best_dt = dt_CV(k = 3, #number of fold\n",
    "        depth_vals = depth_vals, \n",
    "        X_train = clf.clusters[i][0],\n",
    "        y_train = clf.clusters[i][1], \n",
    "        create_plot = None)\n",
    "    clf.clf[i] = best_dt\n",
    "    \n",
    "predictions = clf.predict(X_test)\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 183.0, 'R': 588.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.pred_counts"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
